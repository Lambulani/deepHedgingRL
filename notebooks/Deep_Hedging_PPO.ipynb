{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/levyb/Documents/Masters Data Science - 2nd Year/deepHedgingRL\")\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from src.pricing.asset_price_process import GBM\n",
    "from src.pricing.option_price_process import BSM\n",
    "from src.custom_environments.HedgeEnv_PPO import env_hedging_ppo\n",
    "from src.custom_environments.HedgeEnv_DQN import env_hedging_dqn\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "dt = 1/5\n",
    "T = 10\n",
    "num_steps = int(T/dt)\n",
    "s_0 = float(100)\n",
    "strike_price = s_0\n",
    "sigma = 0.01\n",
    "r = 0\n",
    "\n",
    "def cost(delta_h, multiplier):\n",
    "    TickSize = 0.1\n",
    "    return multiplier * TickSize * (np.abs(delta_h) + 0.01 * delta_h**2)\n",
    "\n",
    "\n",
    "apm = GBM(mu=mu, dt=dt, s_0=s_0, sigma=sigma)\n",
    "opm = BSM(strike_price=strike_price, risk_free_interest_rate=r, volatility=sigma, T=T, dt=dt)\n",
    "\n",
    "\n",
    "env_ppo = env_hedging_ppo(asset_price_model=apm, dt=dt, T=T, num_steps=num_steps, cost_multiplier = 0, tick_size=0.01,\n",
    "                     L=1, strike_price=strike_price, integer_holdings=True, initial_holding=0, mode=\"PL\",\n",
    "                  option_price_model=opm)\n",
    "\n",
    "env_dqn = env_hedging_dqn(asset_price_model=apm, dt=dt, T=T, num_steps=num_steps, cost_multiplier = 0, tick_size=0.01,\n",
    "                     L=1, strike_price=strike_price, integer_holdings=True, initial_holding=0, mode=\"PL\",\n",
    "                  option_price_model=opm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from gym import spaces\n",
    "from typing import Callable, Tuple\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os \n",
    "\n",
    "\n",
    "n_envs = 2\n",
    "vec_env = make_vec_env(lambda:env_hedging_ppo(asset_price_model=apm, dt=dt, T=T, num_steps=num_steps, cost_multiplier = 0, tick_size=0.01,\n",
    "                     L=1, strike_price=strike_price, integer_holdings=True, initial_holding=0, mode=\"PL\",\n",
    "                  option_price_model=opm) , n_envs= n_envs, seed = seed)\n",
    "\n",
    "\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function with 5 hidden layers.\n",
    "    Each layer uses ReLU activation, witorch batch normalization applied before ReLU.\n",
    "    \n",
    "    :param feature_dim: dimension of torche features extracted by the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 201,\n",
    "        last_layer_dim_vf: int = 1,\n",
    "        hidden_dim: int = 128,  # Hidden layer dimension\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network with 5 hidden layers, batch normalization before ReLU\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, last_layer_dim_pi),  # Final layer\n",
    "            nn.ReLU()  # Optional ReLU for the final layer\n",
    "        )\n",
    "\n",
    "        # Value network with 5 hidden layers, batch normalization before ReLU\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, last_layer_dim_vf),  # Final layer\n",
    "            nn.ReLU()  # Optional ReLU for the final layer\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Disable orthogonal initialization\n",
    "        kwargs[\"ortho_init\"] = False\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n",
    "\n",
    "\n",
    "# Custom callback to log rewards during training\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.total_reward = 0.0\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        # Open file for writing average rewards at the start of training\n",
    "        self.file = open(os.path.join(self.log_dir, 'avg_rewards_ppo.txt'), 'w')\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = self.locals[\"rewards\"][0]  # Access the current reward\n",
    "        self.total_reward += reward\n",
    "        self.episode_count += 1\n",
    "\n",
    "        # Log rewards at each check frequency\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Calculate average reward\n",
    "            avg_reward = (self.total_reward / self.episode_count)/n_envs\n",
    "            timestep = self.n_calls * self.locals['env'].num_envs  # Total timesteps processed\n",
    "            self.file.write(f\"{timestep}, {avg_reward}\\n\")\n",
    "            self.file.flush()  # Ensure it's written immediately\n",
    "        return True\n",
    "\n",
    "    # def _on_training_end(self) -> None:\n",
    "    #     # Close the file after training ends\n",
    "    #     self.file.close()\n",
    "\n",
    "# Set up callback and directories for logging and saving models\n",
    "log_dir = \"C:/Users/levyb/Documents/Masters Data Science - 2nd Year/deepHedgingRL/results/PPO/du/logs\"\n",
    "callback = RewardCallback(check_freq=1000, log_dir=log_dir, verbose=1)\n",
    "\n",
    "models_dir = \"C:/Users/levyb/Documents/Masters Data Science - 2nd Year/deepHedgingRL/models/PPO/du\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "\n",
    "policy_kwargs = dict(activation_fn = nn.ReLU, net_arch = [128, 128, 128,128])\n",
    "# Instantiate and train the PPO model as per the Du paper\n",
    "model = PPO(\n",
    "    policy= CustomActorCriticPolicy, \n",
    "    env= vec_env, \n",
    "    learning_rate=1e-4,\n",
    "    n_steps= 1500, #update every 15000 episodes\n",
    "    n_epochs = 5, \n",
    "    clip_range =0.2, \n",
    "    verbose=1,\n",
    "    gae_lambda=0.96, \n",
    "    gamma=0.85,\n",
    "    ent_coef=0.2,  # Entropy coefficient for exploration #check c1 parameter and c2 parameter 0\n",
    "    vf_coef=0.5,  # Value function coefficient\n",
    "    max_grad_norm=1.0,\n",
    "    tensorboard_log=log_dir\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model in increments and save after each block of timesteps\n",
    "TIMESTEPS = 1000000\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\", callback=callback)\n",
    "model.save(f\"{models_dir}/ppo_{TIMESTEPS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# # Calculate moving average to smooth the curve\n",
    "# window_size = 50 # Adjust the window size as needed\n",
    "# smoothed_rewards = np.convolve(callback.rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# # Alternatively, you can use Gaussian smoothing for more flexible control:\n",
    "# # smoothed_rewards = gaussian_filter1d(callback.rewards, sigma=2)\n",
    "\n",
    "# # Adjust train_data_point to match the length of smoothed_rewards\n",
    "# train_data_point = np.arange(len(smoothed_rewards)) * callback.check_freq\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_data_point, smoothed_rewards, label='Reward', color='green')\n",
    "# plt.xlabel('Timesteps', fontsize=14)\n",
    "# plt.ylabel('Reward', fontsize=14)\n",
    "# plt.title('Convergence of PPO - Reward Over Training', fontsize=16)\n",
    "# plt.legend(loc='best')\n",
    "# plt.grid(True)\n",
    "# plt.savefig(\"PPO_Convergence_1500000.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import os \n",
    "\n",
    "\n",
    "model = PPO.load(\n",
    "    \"C:/Users/levyb/Documents/Masters Data Science - 2nd Year/deepHedgingRL/models/PPO/du/PPO_6/ppo_1900000.zip\", \n",
    ")\n",
    "\n",
    "num_episodes = 3\n",
    "action_mapping = np.arange(-100, 101) \n",
    "for episode in range(num_episodes): \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cum_option_pnl = 0\n",
    "    cum_stock_pnl = 0\n",
    "    cum_cost_pnl = 0\n",
    "    option_pnls = []\n",
    "    stock_pnls = []\n",
    "    cost_pnls = []\n",
    "    total_pnl = []\n",
    "    stock_pos_shares =[]\n",
    "    delta_pos_shares = []\n",
    "    \n",
    "    while not done:\n",
    "        current_state_ppo = env_ppo.get_state()\n",
    "        current_state_dqn = env_ppo.get_state()\n",
    "\n",
    "        delta_action_ppo = -100 * round(current_state[4], 2)\n",
    "        delta_action_ppo = -100 * round(current_state[4], 2)\n",
    "        action, _states = model.predict(current_state, deterministic = False)\n",
    "        next_state_ppo, reward, done,done,  info = env_ppo.step(action)\n",
    "        next_state_dqn, reward, done,done,  info = env_dqn.step(action)\n",
    "        delta_h =   next_state[0]-current_state[0]\n",
    "        option_pnl = (100*(next_state[3]-current_state[3]))\n",
    "        stock_pnl = (-current_state[0]*(next_state[1] -current_state[1])-cost(delta_h, 5))\n",
    "        cum_option_pnl =+ option_pnl\n",
    "        cum_stock_pnl =+ stock_pnl\n",
    "        cum_cost_pnl =+ cost(delta_h, 5)\n",
    "\n",
    "        option_pnls.append(cum_option_pnl)\n",
    "        stock_pnls.append(cum_stock_pnl)\n",
    "        cost_pnls.append(cum_cost_pnl)\n",
    "        total_pnl.append(cum_option_pnl+cum_stock_pnl)\n",
    "        stock_pos_shares.append(next_state[0]/10)\n",
    "        delta_pos_shares.append(delta_action)\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "\n",
    "    time_axis = np.linspace(1, num_steps+1, num_steps)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(time_axis, stock_pnls, label='Stock P&L', color='green', linestyle='-')\n",
    "    plt.plot(time_axis, option_pnls, label='Option P&L', color='red', linestyle='--')\n",
    "    #plt.plot(time_axis, cost_pnls, label='Cost P&L', color='magenta', linestyle='--')\n",
    "    plt.plot(time_axis, total_pnl, label='Total P&L', color='black', linestyle='-.')\n",
    "    plt.plot(time_axis, stock_pos_shares, label='Stock Position Shares', color='blue', linestyle=':')\n",
    "    plt.plot(time_axis, delta_pos_shares, label='Delta Position Shares', color='orange', linestyle='--')\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Timestep (D * T)', fontsize=14)\n",
    "    plt.ylabel('Value (Dollars or Shares)', fontsize=14)\n",
    "    plt.title('Cumulative P&L and Positions Over Time', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"PPO_Cost5\")\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Calculate moving average to smooth the curve\n",
    "window_size = 50 # Adjust the window size as needed\n",
    "smoothed_rewards = np.convolve(callback.rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Alternatively, you can use Gaussian smoothing for more flexible control:\n",
    "# smoothed_rewards = gaussian_filter1d(callback.rewards, sigma=2)\n",
    "\n",
    "# Adjust train_data_point to match the length of smoothed_rewards\n",
    "train_data_point = np.arange(len(smoothed_rewards)) * callback.check_freq\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_data_point, smoothed_rewards, label='Reward', color='green')\n",
    "plt.xlabel('Timesteps', fontsize=14)\n",
    "plt.ylabel('Reward', fontsize=14)\n",
    "plt.title('Convergence of PPO - Reward Over Training', fontsize=16)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"PPO_Convergence_1500000.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_t_statistic(data):\n",
    "    n = len(data)\n",
    "    sample_mean = np.mean(data)\n",
    "    sample_std = np.std(data, ddof=1)\n",
    "    t_statistic = sample_mean / (sample_std / np.sqrt(n))\n",
    "    return t_statistic\n",
    "\n",
    "# Getting kernel density estimates for cost and volatility\n",
    "num_episodes = 10000\n",
    "cost_pnls_dh = []\n",
    "cost_pnls_ppo = []\n",
    "total_pnls_vol_dh = []\n",
    "total_pnls_vol_ppo = []\n",
    "t_stat_pnls_dh = []\n",
    "t_stat_pnls_ppo = []\n",
    "\n",
    "for episode in range(num_episodes): \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    deltas = [0]\n",
    "    samp_cost_pnl_dh = 0\n",
    "    samp_cost_pnl_ppo = 0\n",
    "\n",
    "    pnl_diffs_ppo = []\n",
    "    pnl_diffs_dh = []\n",
    "    \n",
    "    previous_pnl_ppo = 0\n",
    "    previous_pnl_dh = 0\n",
    "    \n",
    "    total_pnl_ppo = 0\n",
    "    total_pnl_dh = 0\n",
    "    samp_total_pnl_dh = []\n",
    "    samp_total_pnl_ppo = []\n",
    "\n",
    "    while not done:\n",
    "        current_holdings, current_asset_price, current_ttm, current_option_price, current_delta = env.get_state()\n",
    "        delta_holding = -100 * round(current_delta, 2)\n",
    "        delta_h_dh = delta_holding - deltas[-1]\n",
    "        deltas.append(delta_holding)\n",
    "\n",
    "        action, _states = model.predict(env.get_state(), deterministic=False)\n",
    "        next_state, reward, done,done,  info = env.step(action-0)\n",
    "        next_holdings, next_asset_price, next_ttm, next_option_price, next_delta = next_state\n",
    "        delta_h_ppo = next_holdings-current_holdings\n",
    "\n",
    "        # Cost calculations for both policies\n",
    "        samp_cost_pnl_dh += cost(delta_h_dh, 5)\n",
    "        samp_cost_pnl_ppo += cost(delta_h_ppo, 5)\n",
    "\n",
    "        # P&L differences between steps\n",
    "        pnl_ppo = (100 * (next_option_price - current_option_price) - current_holdings * (next_asset_price - current_asset_price) - cost(delta_h_ppo, 5))\n",
    "        pnl_dh = (100 * (next_option_price - current_option_price) + deltas[-2] * (next_asset_price - current_asset_price) - cost(delta_h_dh, 5))\n",
    "        \n",
    "        pnl_diffs_ppo.append(pnl_ppo - previous_pnl_ppo)\n",
    "        pnl_diffs_dh.append(pnl_dh - previous_pnl_dh)\n",
    "\n",
    "        # Update previous P&L\n",
    "        previous_pnl_ppo = pnl_ppo\n",
    "        previous_pnl_dh = pnl_dh\n",
    "\n",
    "        # Accumulate the cumulative P&L for each policy\n",
    "        total_pnl_ppo+= pnl_ppo\n",
    "        total_pnl_dh += pnl_dh\n",
    "        samp_total_pnl_ppo.append(total_pnl_ppo)\n",
    "        samp_total_pnl_dh.append(total_pnl_dh)\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            # Volatility is the standard deviation of P&L differences\n",
    "            vol_ppo = np.std(pnl_diffs_ppo)\n",
    "            vol_dh = np.std(pnl_diffs_dh)\n",
    "\n",
    "            # Calculate student t-statistic based on cumulative P&L at the end of the episode\n",
    "            t_stat_pnl_ppo = student_t_statistic(samp_total_pnl_ppo)  # Use cumulative P&L\n",
    "            t_stat_pnl_dh = student_t_statistic(samp_total_pnl_dh)  # Use cumulative P&L\n",
    "\n",
    "            # Append cost and volatility for this episode\n",
    "            cost_pnls_ppo.append(samp_cost_pnl_ppo)\n",
    "            cost_pnls_dh.append(samp_cost_pnl_dh)\n",
    "            total_pnls_vol_ppo.append(vol_ppo)\n",
    "            total_pnls_vol_dh.append(vol_dh)\n",
    "            t_stat_pnls_ppo.append(t_stat_pnl_ppo)\n",
    "            t_stat_pnls_dh.append(t_stat_pnl_dh)\n",
    "\n",
    "            state = env.reset()\n",
    "\n",
    "# Plotting the KDE and results\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot kernel density estimates for total cost\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot([x/100 for x in cost_pnls_ppo], label='Policy: ppo', shade=True)\n",
    "sns.kdeplot(cost_pnls_dh, label='Policy: $\\delta_{DH}$', shade=True)\n",
    "plt.title('KDE for Total Cost')\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Plot kernel density estimates for volatility of total P&L\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.kdeplot([x/100 for x in total_pnls_vol_ppo], label='Policy: ppo', shade=True)\n",
    "sns.kdeplot(total_pnls_vol_dh, label='Policy: $\\delta_{DH}$', shade=True)\n",
    "plt.title('KDE for Volatility of Total P&L')\n",
    "plt.xlabel('Volatility of Total P&L')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Plot KDE for Student's t-statistic of Cumulative P&L\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.kdeplot([x/100 for x in t_stat_pnls_ppo], label='Policy: ppo', shade=True)\n",
    "sns.kdeplot([x/100 for x in t_stat_pnls_dh], label='Policy: $\\delta_{DH}$', shade=True)\n",
    "plt.title(\"Student's t-Statistic KDE for Cumulative P&L\")\n",
    "plt.xlabel('Student t-Statistic')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the KDE and results\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot kernel density estimates for total cost\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot([x/100 for x in cost_pnls_ppo], label='Policy: ppo', shade=True)\n",
    "sns.kdeplot(cost_pnls_dh, label='Policy: $\\delta_{DH}$', shade=True)\n",
    "plt.title('KDE for Total Cost')\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Plot kernel density estimates for volatility of total P&L\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.kdeplot([x/100 for x in total_pnls_vol_ppo], label='Policy: ppo', shade=True)\n",
    "sns.kdeplot(total_pnls_vol_dh, label='Policy: $\\delta_{DH}$', shade=True)\n",
    "plt.title('KDE for Volatility of Total P&L')\n",
    "plt.xlabel('Volatility of Total P&L')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Plot KDE for Student's t-statistic of Cumulative P&L\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.kdeplot([x/100 for x in t_stat_pnls_ppo], label='Policy: ppo', shade=True)\n",
    "sns.kdeplot([x/100 for x in t_stat_pnls_dh], label='Policy: $\\delta_{DH}$', shade=True)\n",
    "plt.title(\"Student's t-Statistic KDE for Cumulative P&L\")\n",
    "plt.xlabel('Student t-Statistic')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PPO_Density_Plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the BSM model\n",
    "strike_price = 100\n",
    "env.reset()\n",
    "# Define stock prices for out-of-the-money, at-the-money, and in-the-money scenarios\n",
    "stock_prices = [98, 100, 102] \n",
    "time_to_maturity = 1/5\n",
    "\n",
    "# Initialize arrays to store actions\n",
    "ppo_actions_out_of_money = []\n",
    "ppo_actions_at_money = []\n",
    "ppo_actions_in_money = []\n",
    "\n",
    "\n",
    "stock_positions = np.arange(-20, -100, -1)  # Adjust as necessary\n",
    "deltas_out= np.arange(0.20,1, 0.01)\n",
    "deltas_at = np.arange(-0.30, 0.50, 0.01)\n",
    "deltas_in = np.arange(-0.80,0, 0.01)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(stock_positions)):\n",
    "    # Time to maturity remains constant for this plot\n",
    "    ttm = time_to_maturity  # Time to maturity in years\n",
    "\n",
    "    # Out-of-the-money (S = 98)\n",
    "    S_out = 98\n",
    "    # Compute option price and delta\n",
    "    option_price_out = opm.compute_option_price(ttm, S_out, mode='ttm')\n",
    "    env.set_state(stock_positions[i], S_out, ttm, option_price_out, deltas_out[i])\n",
    "    action_out, _ = model.predict(env.get_state(), deterministic=True)\n",
    "    ppo_actions_out_of_money.append(action_out-100)\n",
    "\n",
    "    # At-the-money (S = 100)\n",
    "    S_at = 100\n",
    "    option_price_at = opm.compute_option_price(ttm, S_at, mode='ttm')\n",
    "    env.set_state(stock_positions[i], S_at, ttm, option_price_at,deltas_at[i])\n",
    "    action_at, _ = model.predict(env.get_state(), deterministic=True)\n",
    "    ppo_actions_at_money.append(action_at-150)\n",
    "\n",
    "\n",
    "    # In-the-money (S = 102)\n",
    "    S_in = 102\n",
    "    option_price_in = opm.compute_option_price(ttm, S_in, mode='ttm')\n",
    "    env.set_state(stock_positions[i], S_in, ttm, option_price_in, deltas_in[i])\n",
    "    action_in, _ = model.predict(env.get_state(), deterministic=True)\n",
    "    ppo_actions_in_money.append(action_in-200)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Out-of-the-money\n",
    "plt.plot(stock_positions, ppo_actions_out_of_money, label=\"ppo Out of Money\", marker=\"*\", linestyle=\"\", color= \"red\")\n",
    "plt.plot(stock_positions, [x*100 for x in deltas_out], label=\"Delta Out of Money\", linestyle=\"--\",color= \"red\")\n",
    "\n",
    "# At-the-money\n",
    "plt.plot(stock_positions, ppo_actions_at_money, label=\"ppo At the Money\", marker=\"o\", linestyle=\"\", color = \"blue\")\n",
    "plt.plot(stock_positions, [x*100 for x in deltas_at], label=\"Delta At the Money\", linestyle=\"--\", color = \"blue\")\n",
    "\n",
    "# In-the-money\n",
    "plt.plot(stock_positions, ppo_actions_in_money, label=\"ppo In the Money\", marker=\"^\", linestyle=\"\", color =\"green\")\n",
    "plt.plot(stock_positions, [x*100 for x in deltas_in], label=\"Delta In the Money\", linestyle=\"--\", color = \"green\")\n",
    "\n",
    "plt.xlim(-20, -100)\n",
    "plt.title(\"Policy Plot for ppo and Delta Hedging\")\n",
    "plt.xlabel(\"Stock Position\")\n",
    "plt.ylabel(\"Actions (Long/Short)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"PPO Policy Plot\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
