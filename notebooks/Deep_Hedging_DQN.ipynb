{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104822e5",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5311da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import option_price_process\n",
    "import HedgeEnv_PPO\n",
    "import HedgeEnv_DQN\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from asset_price_process import GBM\n",
    "from option_price_process import BSM\n",
    "from HedgeEnv import env_hedging\n",
    "from torch import nn\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad347939",
   "metadata": {},
   "source": [
    "### Setting Up Environment, Asset Price model, and Option Pricing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52155e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "dt = 1/5\n",
    "T = 10\n",
    "num_steps = int(T/dt)\n",
    "s_0 = float(100)\n",
    "strike_price = s_0\n",
    "sigma = 0.1\n",
    "r = 0\n",
    "\n",
    "def cost(delta_h, multiplier):\n",
    "    TickSize = 0.1\n",
    "    return multiplier * TickSize * (np.abs(delta_h) + 0.01 * delta_h**2)\n",
    "\n",
    "\n",
    "apm = GBM(mu=mu, dt=dt, s_0=s_0, sigma=sigma)\n",
    "opm = BSM(strike_price=strike_price, risk_free_interest_rate=r, volatility=sigma, T=T, dt=dt)\n",
    "env = HedgeEnv_DQN.env_hedging(asset_price_model=apm, dt=dt, T=T, num_steps=num_steps, cost_multiplier = 0, tick_size=0.01,\n",
    "                     L=1, strike_price=strike_price, int_holdings=True, initial_holding=0, mode=\"PL\",\n",
    "                  option_price_model=opm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c7ce3",
   "metadata": {},
   "source": [
    "### Untrained Agent making random actions\n",
    "\n",
    "We simulate a BSM world, but modified to renact the realities of trading: Discrete time\n",
    "and space. We consider a stock whose price process is a geometric Brownian motion (GBM)\n",
    "with initial price $S_{0}$ and daily lognormal volatility of $\\sigma$/day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c40fe",
   "metadata": {},
   "source": [
    "### DQN Agent using StableBaselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc98161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "import os\n",
    "\n",
    "env = HedgeEnv_DQN.env_hedging(asset_price_model=apm, dt=dt, T=T, num_steps=num_steps, cost_multiplier = 0, tick_size=0.01,\n",
    "                     L=1, strike_price=strike_price, int_holdings=True, initial_holding=0, mode=\"PL\",\n",
    "                  option_price_model=opm)\n",
    "\n",
    "# Custom Feature Extractor with LSTM\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 201):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        # Assuming the input is a 1D vector, reshape it to (batch_size, sequence_length, features)\n",
    "        input_dim = observation_space.shape[0]\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, features_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape the input to add a sequence length of 1\n",
    "        observations = observations.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(observations)\n",
    "        # Use the output of the last time step\n",
    "        last_timestep_output = lstm_out[:, -1, :]\n",
    "        return self.fc_net(last_timestep_output)\n",
    "\n",
    "# Custom DQN Policy\n",
    "class CustomDQNPolicy(DQNPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Discrete,\n",
    "        lr_schedule,\n",
    "        net_arch=None,\n",
    "        activation_fn=nn.ReLU,\n",
    "        features_extractor_class=CustomFeatureExtractor,\n",
    "        features_extractor_kwargs=None,\n",
    "        normalize_images=False,\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs=None,\n",
    "    ):\n",
    "        super(CustomDQNPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn,\n",
    "            features_extractor_class=features_extractor_class,\n",
    "            features_extractor_kwargs=features_extractor_kwargs,\n",
    "            normalize_images=normalize_images,\n",
    "            optimizer_class=optimizer_class,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "# Learning rate schedule\n",
    "    \n",
    "lr_schedule = get_schedule_fn(1e-4)\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, verbose=1):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Save the reward for this step\n",
    "        reward = self.locals[\"rewards\"][0]  # Access the current reward\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            self.rewards.append(reward)  # Log rewards at each check frequency\n",
    "        return True\n",
    "\n",
    "# Usage\n",
    "callback = RewardCallback(check_freq=1000, verbose=1)\n",
    "\n",
    "models_dir = \"models/DQN\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "\n",
    "model_DQN = DQN(policy= CustomDQNPolicy, \n",
    "    env=env,\n",
    "    learning_rate=lr_schedule,\n",
    "    verbose=1,\n",
    "    gamma= 0.85, \n",
    "    batch_size=32,\n",
    "    tensorboard_log= logdir)\n",
    "\n",
    "TIMESTEPS = 3000000\n",
    "model_DQN.learn(total_timesteps= TIMESTEPS, reset_num_timesteps= False, tb_log_name=\"DQN\", callback= callback)\n",
    "model_DQN.save(f\"{models_dir}/{TIMESTEPS}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# num_episodes = 100\n",
    "\n",
    "# for episode in range(num_episodes): \n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     cum_option_pnl = 0\n",
    "#     cum_stock_pnl = 0\n",
    "#     cum_cost_pnl = 0\n",
    "#     option_pnls = [0]\n",
    "#     stock_pnls = [0]\n",
    "#     cost_pnls = [0]\n",
    "#     total_pnl = [0]\n",
    "#     stock_pos_shares =[state[0]]\n",
    "#     delta_pos_shares = [-100*round(state[4], 2)]\n",
    "    \n",
    "#     while not done:\n",
    "#         current_state = env.get_state()\n",
    "#         delta_action = -100 * round(current_state[4], 2)\n",
    "#         action, _states = model.predict(current_state, deterministic = True)\n",
    "#         next_state, reward, done, info = env.step(action-0)\n",
    "#         delta_h =   next_state[0] -current_state[0]\n",
    "#         option_pnl = (100*(next_state[3]-current_state[3]))/100\n",
    "#         stock_pnl = (-next_state[0]*(next_state[1] -current_state[1])- cost(delta_h, 5))/100\n",
    "#         cum_option_pnl += option_pnl\n",
    "#         cum_stock_pnl += stock_pnl\n",
    "#         cum_cost_pnl += cost(delta_h, 5)/100\n",
    "\n",
    "#         option_pnls.append(cum_option_pnl)\n",
    "#         stock_pnls.append(cum_stock_pnl)\n",
    "#         cost_pnls.append(cum_cost_pnl)\n",
    "#         total_pnl.append(cum_option_pnl+cum_stock_pnl)\n",
    "#         stock_pos_shares.append(next_state[0])\n",
    "#         delta_pos_shares.append(delta_action)\n",
    "\n",
    "#         if done:\n",
    "#             state = env.reset()\n",
    "\n",
    "#     time_axis = np.linspace(0, num_steps, num_steps + 1)\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.plot(time_axis, stock_pnls, label='Stock P&L', color='green', linestyle='-')\n",
    "#     plt.plot(time_axis, option_pnls, label='Option P&L', color='red', linestyle='--')\n",
    "#     plt.plot(time_axis, cost_pnls, label='Cost P&L', color='magenta', linestyle='--')\n",
    "#     plt.plot(time_axis, total_pnl, label='Total P&L', color='black', linestyle='-.')\n",
    "#     plt.plot(time_axis, stock_pos_shares, label='Stock Position Shares', color='blue', linestyle=':')\n",
    "#     plt.plot(time_axis, delta_pos_shares, label='Delta Position Shares', color='orange', linestyle='--')\n",
    "\n",
    "#     # Add labels, title, and legend\n",
    "#     plt.xlabel('Timestep (D * T)', fontsize=14)\n",
    "#     plt.ylabel('Value (Dollars or Shares)', fontsize=14)\n",
    "#     plt.title('Cumulative P&L and Positions Over Time', fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.savefig(\"Figure2_Baseline\")\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = DQN.load(\"dqn_hedge\")\n",
    "\n",
    "evaluate_policy(model, env, n_eval_episodes= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_t_statistic(data):\n",
    "    n = len(data)\n",
    "    sample_mean = np.mean(data)\n",
    "    sample_std = np.std(data, ddof=1)\n",
    "    t_statistic = sample_mean / (sample_std / np.sqrt(n))\n",
    "    return t_statistic\n",
    "\n",
    "# Gettting kernel density estimates for cost and volatility \n",
    "num_episodes = 1\n",
    "cost_pnls_dh= []\n",
    "cost_pnls_reinf = []\n",
    "total_pnls_vol_dh =[]\n",
    "total_pnls_vol_reinf = []\n",
    "\n",
    "for episode in range(num_episodes): \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    dh_actions = [0]\n",
    "    cum_cost_pnl_dh = 0\n",
    "    cum_cost_pnl_reinf = 0\n",
    "    cum_total_pnl_dh = []\n",
    "    cum_total_pnl_reinf = [] \n",
    "    deltas =[]\n",
    "    \n",
    "    while not done:\n",
    "        current_state = env.get_state()\n",
    "        delta_action = -100 * round(current_state[4], 2)\n",
    "        deltas.append(delta_action)\n",
    "        delta_h_dh= delta_action - dh_actions[-1]\n",
    "        dh_actions.append(delta_action)\n",
    "\n",
    "        action, _states = model.predict(current_state, deterministic = False)\n",
    "        next_state, reward, done, info = env.step(action-0)\n",
    "        delta_h_reinf =   next_state[0] -current_state[0]\n",
    "        \n",
    "\n",
    "        cum_cost_pnl_dh += cost(delta_h_dh, 5)\n",
    "        cum_cost_pnl_reinf += cost(delta_h_reinf, 5)\n",
    "\n",
    "        total_pnl_reinf = ((100*(next_state[3]-current_state[3])) + (-next_state[0]*(next_state[1] -current_state[1])- cost(delta_h_reinf, 5)))/100\n",
    "        total_pnl_dh = ((100*(next_state[3]-current_state[3])) + (-delta_action*(next_state[1] -current_state[1])- cost(delta_h_dh, 5)))/100\n",
    "        cum_total_pnl_reinf.append(total_pnl_reinf)\n",
    "        cum_total_pnl_dh.append(total_pnl_dh)\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "\n",
    "    vol_reinf = np.std(cum_total_pnl_reinf)\n",
    "    vol_dh = np.std(cum_total_pnl_dh)\n",
    "\n",
    "    cost_pnls_reinf.append(cum_cost_pnl_reinf)\n",
    "    cost_pnls_dh.append(cum_cost_pnl_dh)\n",
    "    total_pnls_vol_reinf.append(vol_reinf)\n",
    "    total_pnls_vol_dh.append(vol_dh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cf26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot kernel density estimates for total cost\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(cost_pnls_dh, label='Policy: $\\delta_{DH}$', shade=True)\n",
    "sns.kdeplot(cost_pnls_reinf, label='Policy: reinf', shade=True)\n",
    "plt.title('KDE for Total Cost')\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Plot kernel density estimates for volatility of total P&L\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(total_pnls_vol_dh, label='Policy: $\\delta_{DH}$', shade=True)\n",
    "sns.kdeplot(total_pnls_vol_reinf, label='Policy: reinf', shade=True)\n",
    "plt.title('KDE for Volatility of Total P&L')\n",
    "plt.xlabel('Volatility of Total P&L')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Figure5_KernelDensities\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daae8b4",
   "metadata": {},
   "source": [
    "### Building a PPO Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7d36c",
   "metadata": {},
   "source": [
    "# Build the Deep Hedging Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d96ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Flatten\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from rl.agents import DQNAgent\n",
    "# from rl.policy import BoltzmannQPolicy\n",
    "# from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_agent(model, actions):\n",
    "#     policy = BoltzmannQPolicy()\n",
    "#     memory = SequentialMemory(limit=50000, window_length=1)\n",
    "#     dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "#                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "#     return dqn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn = build_agent(model, actions)\n",
    "# dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "# dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QNet(nn.Module):\n",
    "#     def __init__(self, state_dim, hidden_dim, action_num):\n",
    "#         super(QNet, self).__init__()\n",
    "\n",
    "#         self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc3 = nn.Linear(hidden_dim, action_num)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         a = torch.relu(self.fc1(state))\n",
    "#         a = torch.relu(self.fc2(a))\n",
    "#         return self.fc3(a)\n",
    "\n",
    "# qnet = QNet(4, 20, num_actions)\n",
    "# qnet_t = QNet(4, 20, num_actions)\n",
    "\n",
    "# dqn = DQN(qnet, qnet_t,\n",
    "#           torch.optim.Adam,\n",
    "#           nn.MSELoss(reduction='sum'), discount=0.8, epsilon_decay=0.999, learning_rate=0.001,\n",
    "#           lr_scheduler=torch.optim.lr_scheduler.StepLR, lr_scheduler_kwargs=[{\"step_size\": 1000*128}])\n",
    "\n",
    "# num_eps = 5000\n",
    "# norm_factor = 10000000\n",
    "\n",
    "\n",
    "# def test_delta(n=10):\n",
    "#     rew = []\n",
    "#     for i in range(n):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         state = state[[0, 1, 2, 4]]\n",
    "#         while not done:\n",
    "#             action = state[3] - env.h\n",
    "#             new_state, reward, done = env.step(action)\n",
    "#             reward = np.sum(reward)\n",
    "#             new_state = new_state[[0, 1, 2, 4]]\n",
    "#             reward = -(reward) ** 2 + 1 / 1000 * reward\n",
    "#             #reward = -(action + env.h - state[3]) ** 2  # remove that\n",
    "#             rew.append(reward)\n",
    "#             state = new_state\n",
    "#     return np.mean(rew)\n",
    "\n",
    "\n",
    "# def test(n=10):\n",
    "#     rew = []\n",
    "#     for i in range(n):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         state = state[[0, 1, 2, 4]]\n",
    "#         while not done:\n",
    "#             out = dqn.act_discrete({\"state\": torch.tensor(state, dtype=torch.float32).unsqueeze(0)})\n",
    "#             action = out.squeeze().detach().numpy() / num_actions\n",
    "#             new_state, reward, done = env.step(action - env.h)\n",
    "#             reward = np.sum(reward)\n",
    "#             new_state = new_state[[0, 1, 2, 4]]\n",
    "#             reward = -(reward) ** 2 + 1 / 1000 * reward\n",
    "#             #reward = -(action + env.h - state[3]) ** 2  # remove that\n",
    "#             if i == 1:\n",
    "#                 print(action, state[3])\n",
    "#             rew.append(reward)\n",
    "#             state = new_state\n",
    "#     return np.mean(rew)\n",
    "\n",
    "# rew = []\n",
    "\n",
    "# for j in range(num_eps):\n",
    "#     print(\"episode: \", j)\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     state = state[[0,1,2,4]]\n",
    "#     while not done:\n",
    "#         out = dqn.act_discrete_with_noise({\"state\": torch.tensor(state, dtype=torch.float32).unsqueeze(0)})\n",
    "#         action = out.squeeze().detach().numpy()/num_actions - env.h\n",
    "#         new_state, reward, done = env.step(action)\n",
    "#         #print(action)\n",
    "#         #print(reward)\n",
    "#         reward = np.sum(reward)\n",
    "#         #print(reward)\n",
    "#         new_state = new_state[[0, 1, 2, 4]]\n",
    "#         #print(state)\n",
    "#         reward = -norm_factor*((reward) ** 2 + 1 / 1000 * reward)\n",
    "#         rew.append(reward)\n",
    "\n",
    "#         dqn.store_transition({\n",
    "#             \"state\": {\"state\": torch.tensor(state, dtype=torch.float32).unsqueeze(0)},\n",
    "#             \"action\": {\"action\": out},\n",
    "#             \"next_state\": {\"state\": torch.tensor(new_state, dtype=torch.float32).unsqueeze(0)},\n",
    "#             \"reward\": float(reward),  # norm factor\n",
    "#             \"terminal\": done\n",
    "#         })\n",
    "#         state = new_state\n",
    "\n",
    "#     if j % 50 == 0 and j != 0:\n",
    "#         print(test(10), test_delta(10))\n",
    "#         print(\"reward: \", np.mean(rew), np.mean(rew)/norm_factor)\n",
    "#         rew = []\n",
    "\n",
    "#     if j > 100:\n",
    "#         for _ in range(int(num_steps)):\n",
    "#             dqn.update()\n",
    "\n",
    "# #dqn.save(\"dqn_model_1000\")\n",
    "\n",
    "# rew_m_l = []\n",
    "# cost_m_l = []\n",
    "# for j in range(100):\n",
    "#     rew = []\n",
    "#     cost_l = []\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     state = state[[0,1,2,4]]\n",
    "#     while not done:\n",
    "#         out = dqn.act_discrete({\"state\": torch.tensor(state, dtype=torch.float32).unsqueeze(0)})\n",
    "#         action = out.squeeze().detach().numpy()/num_actions - env.h\n",
    "#         new_state, reward, done = env.step(action)\n",
    "#         cost = reward[1]\n",
    "#         reward = np.sum(reward)\n",
    "\n",
    "#         new_state = new_state[[0, 1, 2, 4]]\n",
    "#         rew.append(reward)\n",
    "#         cost_l.append(cost)\n",
    "#         state = new_state\n",
    "#     rew_m_l.append(np.sum(rew))\n",
    "#     cost_m_l.append(np.sum(cost_l))\n",
    "# print(\"__________\")\n",
    "# print(rew_m_l)\n",
    "# print(np.mean(rew_m_l), np.std(rew_m_l))\n",
    "# print(\"__________\")\n",
    "# print(cost_m_l)\n",
    "# print(np.mean(cost_m_l), np.std(cost_m_l))\n",
    "\n",
    "# rew_m_l = []\n",
    "# cost_m_l = []\n",
    "# for j in range(100):\n",
    "#     rew = []\n",
    "#     cost_l = []\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     state = state[[0, 1, 2, 4]]\n",
    "#     while not done:\n",
    "#         action = state[3] - env.h\n",
    "#         new_state, reward, done = env.step(action)\n",
    "#         cost = reward[1]\n",
    "#         reward = np.sum(reward)\n",
    "\n",
    "#         new_state = new_state[[0, 1, 2, 4]]\n",
    "#         rew.append(reward)\n",
    "#         cost_l.append(cost)\n",
    "#         state = new_state\n",
    "#     rew_m_l.append(np.sum(rew))\n",
    "#     cost_m_l.append(np.sum(cost_l))\n",
    "# print(\"__________\")\n",
    "# print(rew_m_l)\n",
    "# print(np.mean(rew_m_l), np.std(rew_m_l))\n",
    "# print(\"__________\")\n",
    "# print(cost_m_l)\n",
    "# print(np.mean(cost_m_l), np.std(cost_m_l))\n",
    "# print(\"__________\")\n",
    "# print(tim - time.time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
